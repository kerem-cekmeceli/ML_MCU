{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Includes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from data_paths import get_file_paths_ordered\n",
    "from tf_lite_conversion import convert_to_tf_lite, eval_tf_lite_model\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio\n",
    "from pre_process import choose_tot_slice_len, get_data_tensors, compute_mfccs\n",
    "from models import get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of speakers and the percentage of the available samples to use to consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of speakers and the percentage of the available samples to use to consider:\n",
    "num_speakers = 15\n",
    "model_name = 'test_' + str(num_speakers) + '_spk'\n",
    "dataset_percentage = 1. # 0.85\n",
    "segmentLength=1024\n",
    "print(\"Number of speakers : \", num_speakers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_dir_root = './Models/'+ f'{model_name}/'\n",
    "\n",
    "path_keras_model = model_dir_root + 'KerasModels/'\n",
    "path_tf_lite_nq = model_dir_root + 'TFLiteModelsNonQuantized/'\n",
    "path_tf_lite_q = model_dir_root + 'TFLiteModelsQuantized/'\n",
    "# path_c_model = model_dir_root + 'TFLiteModelsQuantized/'\n",
    "path_figures = model_dir_root + 'Figures/'\n",
    "\n",
    "# Create the directories\n",
    "paths = [path_keras_model, path_tf_lite_nq, path_tf_lite_q, path_figures]\n",
    "for path in paths:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "print('Models will be saved to: ', model_dir_root)\n",
    "\n",
    "\n",
    "model_name_keras = 'keras_'+ model_name + '.h5'\n",
    "model_name_tf_lite_nq = 'tflite_nq_'+ model_name + '.tflite'\n",
    "model_name_tf_lite_q = 'tflite_q_'+ model_name + '.tflite'\n",
    "# model_name_c = 'c_' + model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_train, paths_test, y_train_l, y_test_l, all_paths_l = \\\n",
    "    get_file_paths_ordered(num_speaker=num_speakers, test_ratio=0.2, balanced_dataset=False, \n",
    "                           path_plot_sv=path_figures)\n",
    "\n",
    "slice_len, durations = choose_tot_slice_len(paths=all_paths_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samps = len(all_paths_l)\n",
    "plt.figure(figsize=(10,9))\n",
    "plt.title(\"Sample Lengths\")\n",
    "plt.xlabel(\"Duration [sec]\")\n",
    "plt.ylabel(\"Samples\")\n",
    "plt.barh(np.arange(0, nb_samps, 1), durations, height=0.9, label='samples')\n",
    "plt.grid()\n",
    "plt.axvline(x=slice_len, ymin=0, ymax=nb_samps, color='r', label='slice_length')\n",
    "plt.xlim((0, 35))\n",
    "plt.xticks(np.append(plt.xticks()[0], slice_len))\n",
    "plt.legend()\n",
    "plt.savefig(path_figures + 'slice_len_sel.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_s, x_train, y_train, x_test, y_test = get_data_tensors(paths_train=paths_train, paths_test=paths_test, \n",
    "                                                         y_train_l=y_train_l, y_test_l=y_test_l,\n",
    "                                                         tot_slice_len=slice_len,\n",
    "                                                         used_train_sz_rat=dataset_percentage, \n",
    "                                                         used_test_sz_rat=1.,\n",
    "                                                         segmentLength=segmentLength)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play a random sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "fs_i, audio_data_i = wavfile.read(paths_train[i])\n",
    "display(Audio(audio_data_i, rate=fs_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_low=80.\n",
    "f_up=7600.\n",
    "num_mel_bins=80\n",
    "num_mfcc=13\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    x_train_mfcc = compute_mfccs(x_train, frame_length=segmentLength, sample_rate=f_s, \n",
    "                                lower_edge_hertz=f_low, upper_edge_hertz=f_up,\n",
    "                                num_mel_bins=num_mel_bins, num_mfcc=num_mfcc)\n",
    "    x_test_mfcc  = compute_mfccs(x_test, frame_length=segmentLength, sample_rate=f_s, \n",
    "                                lower_edge_hertz=f_low, upper_edge_hertz=f_up,\n",
    "                                num_mel_bins=num_mel_bins, num_mfcc=num_mfcc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot an example MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "extent = [0, x_train_mfcc[i].shape[0]*segmentLength/fs_i, 0, num_mfcc]\n",
    "plt.imshow(tf.transpose(x_train_mfcc[i], [1, 0, 2]), interpolation=\"nearest\", origin=\"lower\", \n",
    "           aspect=\"auto\", extent=extent, cmap='nipy_spectral')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.ylabel(\"MFCC index\")\n",
    "plt.grid()\n",
    "plt.savefig(path_figures + 'mfcc_example.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = x_train_mfcc\n",
    "test_set = x_test_mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import get_model\n",
    "model_idx = 1\n",
    "input_shape = train_set.shape\n",
    "model = get_model(input_shape=input_shape, nb_classes=num_speakers, model_idx=model_idx)\n",
    "model.build(input_shape=input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "\n",
    "with open(model_dir_root + 'modelsummary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define learning parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 32 #8 # nb of togetherly processed segments(of 1024 samples each) \n",
    "epochs = 150 #40 # nb of back propagations\n",
    "loss_fct = 'sparse_categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model and Fit the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=loss_fct, \n",
    "    optimizer=tf.keras.optimizers.Adam(), \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(path_keras_model + model_name_keras, save_best_only=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=15, min_lr=1.0e-5,verbose=1),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=30, verbose=1),       \n",
    "]\n",
    "\n",
    "history_keras = model.fit(x=train_set, \n",
    "                          y=y_train, \n",
    "                          batch_size=batchSize, \n",
    "                          epochs=epochs, \n",
    "                          validation_split=0.2,\n",
    "                          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test_mfcc)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test_mfcc,  y_test, verbose=2)\n",
    "print('Test accuracy:', test_acc)\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# plt.figure()\n",
    "ax1.plot(history_keras.history['accuracy'], label='Keras-Training')\n",
    "ax1.plot(history_keras.history['val_accuracy'], label='Keras-Validation')\n",
    "ax1.axhline(y=test_acc, xmin=0, xmax=epochs, linestyle='-.', color='g', label='Keras-Test')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim([1/num_speakers, 1])\n",
    "ax2.plot(history_keras.history['lr'], color='y', linestyle='--')\n",
    "ax2.set_ylabel('Learning rate', color='y')\n",
    "ax1.legend(loc='right')\n",
    "plt.grid()\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.savefig(path_figures + 'keras_acc_epochs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# plt.figure()\n",
    "ax1.plot(history_keras.history['loss'], label='Keras-Training')\n",
    "ax1.plot(history_keras.history['val_loss'], label='Keras-Validation')\n",
    "ax1.axhline(y=test_loss, xmin=0, xmax=epochs, linestyle='-.', color='g', label='Keras-Test')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid()\n",
    "ax2.plot(history_keras.history['lr'], color='y', linestyle='--')\n",
    "ax2.set_ylabel('Learning rate', color='y')\n",
    "ax2.grid()\n",
    "ax1.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.title(loss_fct)\n",
    "plt.savefig(path_figures + 'keras_loss_epochs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hard = np.argmax(y_pred, axis=1)\n",
    "fig = plt.figure(figsize=(10,9))\n",
    "cm = confusion_matrix(y_test, y_pred_hard, normalize='true')\n",
    "sn.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.savefig(path_figures + 'keras_cm.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about the saved Kerras Model on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_in_kb = os.path.getsize(path_keras_model+model_name_keras) / 1024\n",
    "print(\"Keras model file : \", model_name_keras)\n",
    "print(\"HDF5 Keras Model size: %d KB\" % h5_in_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Lite Conversion and Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_lite_conversion import convert_to_tf_lite\n",
    "tflite_model_nq, tflite_model_q, sizes_on_disk, comp_rats, legends = \\\n",
    "    convert_to_tf_lite(keras_model=model,\n",
    "                       path_keras_model=path_keras_model + model_name_keras, \n",
    "                       path_tf_lite_nq_model=path_tf_lite_nq + model_name_tf_lite_nq,\n",
    "                       train_set=train_set, \n",
    "                       path_tf_lite_q_model=path_tf_lite_q + model_name_tf_lite_q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the disk size comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots for comparison\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "width=0.25\n",
    "x_ax_tmp = np.arange(len(legends))\n",
    "\n",
    "ax1.bar(x_ax_tmp, sizes_on_disk, color='b', width=width, edgecolor='k', label='Size')\n",
    "ax1.set_ylabel(\"Sizes of the models [KB]\", color='b')\n",
    "ax1.legend(loc='upper center')\n",
    "\n",
    "ax2.bar(x_ax_tmp+width, comp_rats, color='g', width=width, edgecolor='k', label='Compression')\n",
    "ax2.set_ylabel(\"Compression Ratios\", color='g')\n",
    "ax2.legend(loc='center', bbox_to_anchor=(0.52, 0.85))\n",
    "\n",
    "plt.grid()\n",
    "plt.title(\"Disk Size Comparisons\")\n",
    "plt.xticks(x_ax_tmp+width/2, legends)\n",
    "plt.savefig(path_figures + 'size_comps.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation Non-Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nq = eval_tf_lite_model(path_tf_lite_model=path_tf_lite_nq + model_name_tf_lite_nq,\n",
    "                             test_set=test_set, \n",
    "                             quantized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_nq_score = accuracy_score(y_test, pred_nq)\n",
    "print(\"Accuracy of non-quantized tflite model is {}%\".format(tflite_nq_score*100))\n",
    "print(\"Compared to float32 accuracy of {}%\".format(test_acc*100))\n",
    "print(\"We have a change of {}%\".format((tflite_nq_score-test_acc)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,9))\n",
    "cm = confusion_matrix(y_test, pred_nq, \n",
    "                      normalize='true')\n",
    "sn.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.savefig(path_figures + 'tflite_nq_cm.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_q = eval_tf_lite_model(path_tf_lite_model=path_tf_lite_q + model_name_tf_lite_q,\n",
    "                            quantized=True, \n",
    "                            test_set=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_q_score = accuracy_score(y_test, pred_q)\n",
    "print(\"Accuracy of quantized to int8 model is {}%\".format(tflite_q_score*100))\n",
    "print(\"Compared to float32 accuracy of {}%\".format(test_acc*100))\n",
    "print(\"We have a change of {}%\".format((tflite_q_score-test_acc)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,9))\n",
    "cm = confusion_matrix(y_test, pred_q, \n",
    "                      normalize='true')\n",
    "sn.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.savefig(path_figures + 'tflite_q_cm.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots for comparison\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "width=0.25\n",
    "x_ax_tmp = np.arange(len(legends))\n",
    "\n",
    "ax1.bar(x_ax_tmp, sizes_on_disk, color='b', width=width, edgecolor='k', label='Size')\n",
    "ax1.set_ylabel(\"Sizes of the models [KB]\", color='b')\n",
    "ax1.legend(loc='upper right',  bbox_to_anchor=(0.77, 0.73))\n",
    "\n",
    "ax2.bar(x_ax_tmp+width, [test_acc, tflite_nq_score, tflite_q_score], \n",
    "        color='g', width=width, edgecolor='k', label='Accuracy')\n",
    "ax2.set_ylabel(\"Accuracy\", color='g')\n",
    "ax2.set_ylim([0., 1.])\n",
    "ax2.axhline(y=test_acc, xmin=0., xmax=1., linestyle='-.', color='r', label='KerasAcc')\n",
    "ax2.legend(loc='upper right',  bbox_to_anchor=(0.85, 0.85))\n",
    "\n",
    "plt.grid()\n",
    "plt.title(\"Accuracy vs Size Comparisons\")\n",
    "plt.xticks(x_ax_tmp+width/2, legends)\n",
    "plt.savefig(path_figures + 'acc_comps.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "9138d4114e08645198299aaf8e2923ca19654615259edf66307bf57fbbcdfa7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
